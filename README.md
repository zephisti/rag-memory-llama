# ğŸ§  Reflective AI Research Assistant

A local-first, intelligent research assistant that:
- Ingests PDFs
- Answers context-aware questions
- Summarizes your learnings
- Scores insights by impact
- Suggests gaps in your understanding
- Evolves with every question you ask

---

## ğŸ“Œ Project Overview

This system helps you **upload research**, **ask questions**, and **track learning** over time using local LLaMA 3.2 (via [Ollama](https://ollama.com)). It stores every answer, analyzes its quality, and lets you reflect on your learning history with analytics, memory chunking, and meta-questioning agents.

---

## ğŸ”§ Features

### ğŸ“ PDF Upload & Chunking
- Drag-and-drop PDF upload from a GUI
- Extracted text is broken into chunks and stored in `data/text_chunks/`

### ğŸ¤– Local Question Answering
- Ask questions using the GUI or CLI
- Retrieves relevant chunks from your research
- Sends to local `llama3` model using `ollama run`
- Displays answer, summary, and auto-generated **impact score**

### ğŸ§  Dual Memory Architecture
- `memory.json`: logs full Q&A history with timestamp, score, and summary
- `memory_chunks.json`: stores sub-chunks of summaries and answers for granular search

### ğŸ“Š Analytics Dashboard
- Run `analytics_dashboard.py` to generate:
  - Topic frequency chart (`topic_frequency.png`)
  - Markdown report (`analytics_report.md`)
  - Highlighted top-impact questions

### ğŸ§ª Reflection & Filtering
- Run `reflect_by_topic.py` to:
  - Group learnings by topic
  - Filter by `--score-threshold`
  - Show `--show-top` insights
- Helps identify high-value vs low-value content

### ğŸ§  Gap Analyzer (Meta Agent)
- Run `gap_analyzer.py` to:
  - Find neglected or stale topics
  - Suggest open-ended questions you havenâ€™t asked
  - Propose next steps to deepen your understanding

---

## ğŸ“ Folder Structure

```
rag-memory-llama/
â”œâ”€â”€ app_gui.py
â”œâ”€â”€ main_cli.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ pdf_parser.py
â”‚   â”œâ”€â”€ memory_manager.py
â”‚   â”œâ”€â”€ memory_chunker.py
â”‚   â”œâ”€â”€ analytics_dashboard.py
â”‚   â”œâ”€â”€ reflect_by_topic.py
â”‚   â”œâ”€â”€ gap_analyzer.py
â”‚   â””â”€â”€ ollama_interface.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/
â”‚   â”œâ”€â”€ text_chunks/
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ memory.json
â”‚   â”‚   â”œâ”€â”€ memory_chunks.json
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ analytics_report.md
â”‚   â”‚   â””â”€â”€ topic_frequency.png
```

---

## âš™ï¸ Setup

### Prerequisites
- Python 3.8+
- [Ollama](https://ollama.com) with `llama3` model installed

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Launch Ollama
```bash
ollama serve
```

### Test Ollama Works
```bash
ollama run llama3 "Hello!"
```

---

## ğŸš€ How to Use

### 1. Run the GUI
```bash
python app_gui.py
```

- Upload PDFs
- Ask questions
- View model responses
- All memory is stored automatically

### 2. Ask from CLI
```bash
python main_cli.py
```

- Same capabilities as the GUI but in terminal form

### 3. Reflect on Your Learning
```bash
python scripts/reflect_by_topic.py --score-threshold 7 --show-top
```

### 4. Generate Analytics
```bash
python scripts/analytics_dashboard.py
```

### 5. Find Gaps in Understanding
```bash
python scripts/gap_analyzer.py --mode all
```

---

## ğŸ’¡ Impact Score: What It Means

Each answer gets an `impact_score` from 0â€“1000, based on:
- Relevance to your question
- Clarity and completeness
- Insightfulness or novelty

Use this score to filter whatâ€™s worth reviewing, reflecting on, or improving.

---

## ğŸ¤– Codex-Driven Development

You used [GitHub Copilot/Codex](https://openai.com/blog/copilot) to:
- Auto-fix bugs in reflection scripts
- Generate patch suggestions
- Complete and merge GitHub pull requests
- Expand core logic using your own prompts

This means the system is **self-extensible** â€” you can continue growing it by prompting Codex from inside GitHub.

---

## ğŸ“ˆ Future Roadmap

- [ ] Weekly Digest Generator (`weekly_digest.py`)
- [ ] Flashcard Export (`flashcards.md`)
- [ ] Semantic Search over All Chunks
- [ ] Codex Planner Agent (auto-asks 1 new question/day)
- [ ] Multi-user SaaS mode with shared knowledge base
- [ ] Notion/Obsidian sync or export

---

## ğŸ§‘â€ğŸ’» Author

Built by [zephisti](https://github.com/zephisti) using:
- Ollama + LLaMA 3.2
- Python (Tkinter, JSON, matplotlib)
- GitHub Copilot & Codex agents

---

## ğŸ“œ License

MIT â€” Educational and experimental.

---

> âœ¨ â€œIt doesnâ€™t just answer your questions â€” it remembers, improves, and reflects on them with you.â€

